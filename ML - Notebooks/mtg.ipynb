{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OczZyO1XiCTL"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG1ugncDb6nR"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y tensorflow keras tensorflow-addons\n",
        "!pip install \"tensorflow==2.15.0\" \"keras==2.15.0\" \"tensorflow-addons==0.23.0\" opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3SMTGv4bxox"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuración\n",
        "set_code = \"fdn\"\n",
        "base_dir = \"mtg_foundations\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "def sanitize_name(name):\n",
        "    \"\"\"Crea nombres seguros para carpetas\"\"\"\n",
        "    return re.sub(r'[^a-zA-Z0-9]', '_', name)[:50]\n",
        "\n",
        "# Consultar API\n",
        "url = f\"https://api.scryfall.com/cards/search?q=e:{set_code}\"\n",
        "while url:\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    for card in tqdm(data.get('data', []), desc=\"Descargando\"):\n",
        "        card_name = card['name']\n",
        "        safe_name = sanitize_name(card_name)\n",
        "        class_dir = os.path.join(base_dir, safe_name)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "        # Obtener URL de imagen\n",
        "        image_url = card.get('image_uris', {}).get('png', '')\n",
        "        if not image_url:\n",
        "            continue\n",
        "\n",
        "        # Generar nombre único\n",
        "        file_ext = image_url.split('.')[-1].split('?')[0]\n",
        "        file_name = f\"{safe_name}_{card['collector_number']}.{file_ext}\"\n",
        "        file_path = os.path.join(class_dir, file_name)\n",
        "\n",
        "        # Descargar solo si no existe\n",
        "        if not os.path.exists(file_path):\n",
        "            try:\n",
        "                response = requests.get(image_url, stream=True, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    with open(file_path, 'wb') as f:\n",
        "                        for chunk in response.iter_content(1024):\n",
        "                            f.write(chunk)\n",
        "            except Exception as e:\n",
        "                print(f\"Error en {card_name}: {e}\")\n",
        "\n",
        "    url = data.get('next_page')\n",
        "\n",
        "print(f\"\\n✅ Estructura creada en: {base_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PKDyUONDay51"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, applications\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# Configuración\n",
        "DATA_DIR = \"/content/mtg_foundations\"\n",
        "AUG_DIR = \"/content/augmented_dataset\"\n",
        "IMG_SIZE = (512, 512)\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS_PER_GROUP = 10  # Reducir épocas para cumplir con el límite de tiempo\n",
        "AUG_PER_CLASS = 1000  # Número de imágenes aumentadas por clase\n",
        "NUM_THREADS = 8  # Número de hilos para multithreading\n",
        "GROUP_SIZE = 44  # Número de clases por grupo\n",
        "\n",
        "# 1. Aumentación de Datos Brutal\n",
        "class CardAugmenter:\n",
        "    def __init__(self, original_size=(600, 840)):\n",
        "        self.original_size = original_size\n",
        "\n",
        "    def random_perspective(self, img):\n",
        "        pts1 = np.float32([[0,0],[600,0],[0,840],[600,840]])\n",
        "        pts2 = np.float32([[np.random.randint(-50,50), np.random.randint(-50,50)],\n",
        "                          [np.random.randint(550,650), np.random.randint(-50,50)],\n",
        "                          [np.random.randint(-50,50), np.random.randint(790,890)],\n",
        "                          [np.random.randint(550,650), np.random.randint(790,890)]])\n",
        "        M = cv2.getPerspectiveTransform(pts1, pts2)\n",
        "        return cv2.warpPerspective(img, M, self.original_size)\n",
        "\n",
        "    def augment_image(self, img):\n",
        "        img = self.random_perspective(img)\n",
        "        img = tf.image.random_brightness(img, 0.4)\n",
        "        img = tf.image.random_contrast(img, 0.5, 1.5)\n",
        "        img = tf.image.random_hue(img, 0.08)\n",
        "        img = tf.image.random_saturation(img, 0.6, 1.6)\n",
        "        img = tf.image.random_jpeg_quality(img, 30, 100)\n",
        "        img = tf.image.random_crop(img, size=[500, 500, 3])\n",
        "        img = tf.image.resize_with_pad(img, *self.original_size)\n",
        "        img = tf.image.resize_with_crop_or_pad(img, 600, 840)\n",
        "        if np.random.rand() > 0.5:\n",
        "            img = tfa.image.gaussian_filter2d(img, filter_shape=(3,3), sigma=1.0)\n",
        "        return img.numpy()\n",
        "\n",
        "# 2. Generar dataset aumentado para un grupo específico\n",
        "def generate_augmented_dataset_for_group(class_dirs):\n",
        "    augmenter = CardAugmenter()\n",
        "\n",
        "    def process_image(class_dir, img, i):\n",
        "        aug_img = augmenter.augment_image(img)\n",
        "        cv2.imwrite(os.path.join(AUG_DIR, class_dir, f\"aug_{i}.jpg\"),\n",
        "                    cv2.cvtColor(aug_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    for class_dir in tqdm(class_dirs, desc=\"Procesando clases\"):\n",
        "        orig_img_path = os.path.join(DATA_DIR, class_dir, os.listdir(os.path.join(DATA_DIR, class_dir))[0])\n",
        "        img = cv2.imread(orig_img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        os.makedirs(os.path.join(AUG_DIR, class_dir), exist_ok=True)\n",
        "        cv2.imwrite(os.path.join(AUG_DIR, class_dir, \"original.jpg\"), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
        "            futures = []\n",
        "            for i in range(AUG_PER_CLASS):\n",
        "                futures.append(executor.submit(process_image, class_dir, img, i))\n",
        "\n",
        "            for future in tqdm(futures, desc=f\"Aumentando {class_dir}\", leave=False):\n",
        "                future.result()\n",
        "\n",
        "# 3. Dividir las clases en grupos\n",
        "all_classes = sorted(os.listdir(DATA_DIR))\n",
        "num_groups = len(all_classes) // GROUP_SIZE + (1 if len(all_classes) % GROUP_SIZE != 0 else 0)\n",
        "groups = [all_classes[i * GROUP_SIZE:(i + 1) * GROUP_SIZE] for i in range(num_groups)]\n",
        "\n",
        "# 4. Entrenar submodelos\n",
        "submodels = []\n",
        "for group_idx, group in enumerate(groups):\n",
        "    print(f\"\\nEntrenando submodelo para el grupo {group_idx + 1}/{len(groups)}\")\n",
        "\n",
        "    # Generar datos aumentados para este grupo\n",
        "    generate_augmented_dataset_for_group(group)\n",
        "\n",
        "    # Cargar dataset aumentado\n",
        "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        AUG_DIR,\n",
        "        validation_split=0.2,\n",
        "        subset=\"training\",\n",
        "        label_mode=\"categorical\",\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        AUG_DIR,\n",
        "        validation_split=0.2,\n",
        "        subset=\"validation\",\n",
        "        label_mode=\"categorical\",\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # Construir y entrenar el submodelo\n",
        "    model = build_model(len(train_ds.class_names))\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-4),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=EPOCHS_PER_GROUP\n",
        "    )\n",
        "\n",
        "    # Guardar el submodelo\n",
        "    submodel_path = f\"submodel_group_{group_idx + 1}.keras\"\n",
        "    model.save(submodel_path)\n",
        "    submodels.append(model)\n",
        "\n",
        "# 5. Combinar los submodelos en un modelo final\n",
        "def combine_submodels(submodels, num_classes_total):\n",
        "    inputs = layers.Input(shape=(*IMG_SIZE, 3))\n",
        "    outputs = []\n",
        "\n",
        "    for submodel in submodels:\n",
        "        submodel.trainable = False  # Congelar los submodelos\n",
        "        outputs.append(submodel(inputs))\n",
        "\n",
        "    combined_output = layers.Concatenate()(outputs)\n",
        "    final_output = layers.Dense(num_classes_total, activation=\"softmax\")(combined_output)\n",
        "\n",
        "    combined_model = models.Model(inputs=inputs, outputs=final_output)\n",
        "    combined_model.compile(\n",
        "        optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-5),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return combined_model\n",
        "\n",
        "# Crear el modelo combinado\n",
        "combined_model = combine_submodels(submodels, num_classes_total=len(all_classes))\n",
        "\n",
        "# Entrenar el modelo combinado si es necesario\n",
        "print(\"\\nEntrenando el modelo combinado...\")\n",
        "train_ds_full = tf.keras.utils.image_dataset_from_directory(\n",
        "    AUG_DIR,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "val_ds_full = tf.keras.utils.image_dataset_from_directory(\n",
        "    AUG_DIR,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "combined_model.fit(\n",
        "    train_ds_full,\n",
        "    validation_data=val_ds_full,\n",
        "    epochs=5  # Pocas épocas para ajustar el modelo combinado\n",
        ")\n",
        "\n",
        "# Guardar el modelo final\n",
        "combined_model.save(\"final_combined_model.keras\")\n",
        "print(\"Modelo combinado guardado como 'final_combined_model.keras'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}